{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "32541f67",
      "metadata": {},
      "source": [
        "## Section 0. Environmental Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53f3739e",
      "metadata": {},
      "source": [
        "### Step A. Check Tensorflow Version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a6f445a",
      "metadata": {},
      "source": [
        "針對環境，本次作業會採用 TensorFlow 進行操作，根據 Google Tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f9f026ea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow Version:  2.18.0\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Numbers of GPU Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
        "print(\"Tensorflow Version: \", tf.__version__)\n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "print(\"Numbers of GPU Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d41d22",
      "metadata": {},
      "source": [
        "### Step B. Check GPU Availability and Set VRAM Growth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c508390d",
      "metadata": {},
      "source": [
        "TensorFlow takes up all available VRAM upon startup by default, which can potentially trigger an OOM error or interfere with other system graphics applications. Consequently, in this implementation, **`memory_growth`** is set to **`True`**. This configuration allows VRAM to be allocated dynamically by TensorFlow based on actual demand, instead of immediately hogging the full **8 GiB** of space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "51959725",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: /physical_device:GPU:0  ; Type: GPU\n"
          ]
        }
      ],
      "source": [
        "print(\"Name:\", gpu.name, \" ; Type:\", gpu.device_type)\n",
        "tf.config.experimental.set_memory_growth(gpu, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30a79c60",
      "metadata": {},
      "source": [
        "## Section 1. Data Engineering and Statistical Verification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b17dbcf0",
      "metadata": {},
      "source": [
        "> _The following procedure is structured in the form of Casella & Berger (2001) Statistical Inference.\n",
        "\n",
        "Define $\\mathcal{D} = \\{ (x_i, y_i, s_i) \\}_{i=1}^N$ as the complete dataset, specifically the DataFrame `df`, where:\n",
        "\n",
        "- $x_i \\in \\mathcal{X}$ is the $i$-th MRI slice image (Tensor / Array).\n",
        "- $y_i \\in \\mathcal{Y}$ is the class label `class_name`, where $\\mathcal{Y} = \\{ \\text{ND, VMD, MD, MOD} \\}$.\n",
        "- $s_i \\in \\mathcal{S}$ is the subject identifier `sid`, representing patient $P$.\n",
        "- $f(x)$ is the joint probability density function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4703c3a8",
      "metadata": {},
      "source": [
        "### Step A. Metadata of the DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "661db48b",
      "metadata": {},
      "source": [
        "Given the directory structure (excluding images), where `work.ipynb` is the Jupyter Notebook for this operation:\n",
        "\n",
        "```text\n",
        ".\n",
        "├── README.md\n",
        "├── work.ipynb\n",
        "└── oasis_data\n",
        "    ├── dementia_mild\n",
        "    ├── dementia_moderate\n",
        "    ├── dementia_very_mild\n",
        "    └── non-demented\n",
        "\n",
        "6 directories, 2 files\n",
        "```\n",
        "\n",
        "Since the filenames in the OASIS dataset are typically structured as `OAS1_0028_MR1_mpr-1_100.jpg` with `OAS1_0028` being the Subject ID,it is first necessary to maintain the original structure while extracting the ID using a regexp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a8d4d1e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_sid(filename):\n",
        "    match = re.search(r\"(OAS\\d+_\\d+)\", filename)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        return \"Unknown\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef05e174",
      "metadata": {},
      "source": [
        "Next, scanning of all files can be performed, and the DataFrame `df` subsequently created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "56420900",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               file_path  \\\n",
            "0      oasis_data/non-demented/OAS1_0001_MR1_mpr-1_10...   \n",
            "1      oasis_data/non-demented/OAS1_0001_MR1_mpr-1_10...   \n",
            "2      oasis_data/non-demented/OAS1_0001_MR1_mpr-1_10...   \n",
            "3      oasis_data/non-demented/OAS1_0001_MR1_mpr-1_10...   \n",
            "4      oasis_data/non-demented/OAS1_0001_MR1_mpr-1_10...   \n",
            "...                                                  ...   \n",
            "74971  oasis_data/dementia_moderate/OAS1_0351_MR1_mpr...   \n",
            "74972  oasis_data/dementia_moderate/OAS1_0351_MR1_mpr...   \n",
            "74973  oasis_data/dementia_moderate/OAS1_0351_MR1_mpr...   \n",
            "74974  oasis_data/dementia_moderate/OAS1_0351_MR1_mpr...   \n",
            "74975  oasis_data/dementia_moderate/OAS1_0351_MR1_mpr...   \n",
            "\n",
            "                         file_name        sid         class_name  \n",
            "0      OAS1_0001_MR1_mpr-1_100.jpg  OAS1_0001       non-demented  \n",
            "1      OAS1_0001_MR1_mpr-1_101.jpg  OAS1_0001       non-demented  \n",
            "2      OAS1_0001_MR1_mpr-1_102.jpg  OAS1_0001       non-demented  \n",
            "3      OAS1_0001_MR1_mpr-1_103.jpg  OAS1_0001       non-demented  \n",
            "4      OAS1_0001_MR1_mpr-1_104.jpg  OAS1_0001       non-demented  \n",
            "...                            ...        ...                ...  \n",
            "74971  OAS1_0351_MR1_mpr-4_156.jpg  OAS1_0351  dementia_moderate  \n",
            "74972  OAS1_0351_MR1_mpr-4_157.jpg  OAS1_0351  dementia_moderate  \n",
            "74973  OAS1_0351_MR1_mpr-4_158.jpg  OAS1_0351  dementia_moderate  \n",
            "74974  OAS1_0351_MR1_mpr-4_159.jpg  OAS1_0351  dementia_moderate  \n",
            "74975  OAS1_0351_MR1_mpr-4_160.jpg  OAS1_0351  dementia_moderate  \n",
            "\n",
            "[74976 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "CLASSES = [\"non-demented\", \"dementia_very_mild\", \"dementia_mild\", \"dementia_moderate\"]\n",
        "DATA_ROOT = \"oasis_data\"\n",
        "\n",
        "data_list = []\n",
        "\n",
        "for label in CLASSES:\n",
        "    files = glob.glob(os.path.join(DATA_ROOT, label, \"*.jpg\"))\n",
        "    \n",
        "    for file_path in files:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        sid = extract_sid(file_name)\n",
        "        data_list.append({\n",
        "            \"file_path\": file_path,\n",
        "            \"file_name\": file_name,\n",
        "            \"sid\": sid,\n",
        "            \"class_name\": label\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(data_list)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26481675",
      "metadata": {},
      "source": [
        "Now let's check the data.\n",
        "\n",
        "1. Check the total number of images and subjects.\n",
        "2. Check the distribution of images per class.\n",
        "3. Check the distribution of subjects per class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cbe6714",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Images: 74976\n",
            "Total Subjects: 300\n",
            "\n",
            "Images per Class: \n",
            " class_name\n",
            "non-demented          67222\n",
            "dementia_mild          5002\n",
            "dementia_very_mild     2264\n",
            "dementia_moderate       488\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Images per Subject (Top 5): \n",
            " sid\n",
            "OAS1_0379    488\n",
            "OAS1_0353    488\n",
            "OAS1_0061    488\n",
            "OAS1_0368    488\n",
            "OAS1_0285    488\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# 1. Check the total number of images and subjects\n",
        "print(f\"Total Images: {len(df)}\")\n",
        "print(f\"Total Subjects: {df['sid'].nunique()}\")\n",
        "print()\n",
        "\n",
        "# 2. Check the distribution of images per class\n",
        "print(\"Images per Class: \\n\", df['class_name'].value_counts())\n",
        "print()\n",
        "\n",
        "# 3. Check the distribution of images per subject\n",
        "print(\"Images per Subject (Top 5): \\n\", df['sid'].value_counts().head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ff8095",
      "metadata": {},
      "source": [
        "### Step B. Establishing the i.i.d. Condition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b146f9c",
      "metadata": {},
      "source": [
        "In **Classical Statistics** (1700s–1960s) for parameter estimation, common methods include **Maximum Likelihood Estimation (MLE)**, **Point Estimation (PE)**, **Bayesian Estimation (BE)**, **Uniformly Most Powerful Test (UMP)**, **Uniformly Minimum Variance Unbiased Estimator (UMVUE)**, and so on. In principle, all statistical inferences are **entirely built upon the premise that \"the data are i.i.d.\"**\n",
        "\n",
        "> **Definition 5.1.1** Identical and Independent Distribution\n",
        ">\n",
        "> The random variables $\\{X_i\\}_{i=1}^{n}$ are called a **random sample** of size $n$ from the **population** $f(x)$ if $\\{X_i\\}_{i=1}^{n}$ are mutually independent random variables and the marginal **pdf** or **pmf** of each $X_i$ is the same function $f(x)$.\n",
        ">\n",
        "> Alternatively, $\\{X_i\\}_{i=1}^{n}$ are called **independent and identically distributed** random variables with **pdf** or **pmf** $f(x)$. This is commonly abbreviated to **i.i.d.** random variables.\n",
        ">\n",
        "\n",
        "This **i.i.d.** assumption makes the optimization formulas very tractable mathematically. Furthermore, when the sample size is sufficiently large, the Central Limit Theorem dictates that the optimization approach most likely to yield the maximum likelihood is statistically correct. In other words, when sampling from some (joint) probability distribution $P$, as long as the **i.i.d.** assumption holds, the **Law of Large Numbers** and **Uniform Convergence** will be satisfied.\n",
        "\n",
        "Today, due to the increasing complexity of input data, the true distribution of the data is often difficult to ascertain, and a large portion of data will violate the **i.i.d.** assumption. Even if the distribution family is known, calculation is challenging under extremely high-dimensional parameter spaces, causing all classical theories to fail. Consequently, a method is needed that **does not require assuming a data distribution and can perform optimization solely based on the data itself**. The concept proposed by Vapnik, V. N., & Chervonenkis, A. Y. (1971), **Empirical Risk Minimization (ERM)**, is fundamentally based on the idea that:\n",
        "\n",
        "> *Since the true joint probability distribution $P(X, Y)$ is unknown, under the condition of having a large amount of sample data, **minimizing the average error on the samples** can be used as an alternative optimization objective.*\n",
        ">\n",
        "\n",
        "By definition, let the input space be $\\mathcal{X}$ and the output space be $\\mathcal{Y}$, and assume they follow some unknown joint probability distribution $P(X, Y)$. Given a loss function $L(f(x), y)$, the theoretical goal of machine learning is to find a function $f^*$ that minimizes the **Expected Risk (True Risk)**:\n",
        "\n",
        "$$\n",
        "R(f) = E_{(x,y) \\sim P}[L(f(x), y)] = \\int L(f(x), y) \\cdot dP(x, y)\n",
        "$$\n",
        "\n",
        "However, $R(f)$ cannot be directly computed because $P(X, Y)$ is unknown. Therefore, based on the **ERM principle**, the practical approach is to optimize the following objective function (including a regularization term):\n",
        "\n",
        "$$\n",
        "\\hat{f} = \\arg\\min_f \\left[ \\underbrace{\\frac{1}{n}\\sum L(f(x_i),y_i)}_{\\text{empirical risk}} + \\underbrace{R(f)}_{\\text{regularizer}}\\right]\n",
        "$$\n",
        "\n",
        "Here, $R(f)$ is the **regularizer**, which can also be realized through **Implicit Regularization** in modern deep learning. Considering current CPU computational resources, the majority of mainstream supervised machine learning algorithms, including **Linear Regression**, **Logistic Regression**, **SVM**, and **Neural Networks** combined with the Adam optimizer, almost universally adopt **Empirical Risk Minimization** (or its Structural Risk Minimization form) as the optimization objective during training. With current computational resources, ERM serves as a **unified training principle** applicable to models ranging from hundreds of parameters to trillions of parameters (e.g., 1.8T parameters).\n",
        "\n",
        "To ensure that $\\hat{R}_n(f)$ is a **valid estimator** of $R(f)$, one must rely on the **Weak Law of Large Numbers (WLLN)** for justification. According to:\n",
        "\n",
        "> **Theorem 5.5.2** Weak Law of Large Number\n",
        ">\n",
        "> Let $\\{X_i\\}_{i=1}^{n}$ be **i.i.d. random variables** with $E[X_i] = \\mu$ and $\\mathrm{Var}[X_i] = \\sigma^2 < \\infty$. Define $\\bar{X}_n = (1/n)\\sum_{i=1}^n X_i$. Then\n",
        ">\n",
        "> $$\\forall \\: \\epsilon > 0, \\quad \\lim_{n \\to \\infty} P(|\\bar{X} - \\mu| < \\epsilon) = 1$$\n",
        ">\n",
        ">\n",
        "> that is, $\\bar{X}_n$ **converges in probability** to $\\mu$.\n",
        "\n",
        "Based on this definition, the loss function $L(f(x), y)$ in machine learning can be viewed as a random variable $Z$. If the sample data $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$ satisfies the **i.i.d.** condition, then the array of loss values $L(f(x_1), y_1), \\dots, L(f(x_n), y_n)$ are also **independent and identically distributed**. In this case, according to the **WLLN**, the empirical risk will converge in probability to the expected risk:\n",
        "\n",
        "$$\n",
        "\\frac{1}{n}\\sum_{i=1}^n L(f(x_i), y_i) \\xrightarrow{P} E_{(x,y) \\sim P}[L(f(x), y)]\n",
        "$$\n",
        "\n",
        "In other words, provided the **i.i.d.** assumption holds, **ERM** is equivalent to minimizing the model's generalization error on the true distribution. Conversely, if the **i.i.d.** assumption is violated, the **WLLN** fails, the empirical risk is **no longer a consistent estimator** of the expected risk, and **all statistical guarantees** of **ERM** become invalid for the model's training results. Hurlbert, S. H. (1984) mentions that this type of issue belongs to **Simple Pseudoreplication**, where there is only one experimental unit (replicate) per treatment, but multiple samples are drawn from that unit and treated as independent repeats for statistical testing.\n",
        "\n",
        "Therefore, the correct sampling unit must be the **single subject** $s \\in \\mathcal{S}$ as the experimental unit. We must assume that different subjects satisfy independence, $\\mathrm{Cov}(s_i, s_j) = 0$ for $i \\neq j$, and design the data splitting process based on this. Based on this definition, if the dataset $\\mathcal{D} = \\{X_i\\}_{i=1}^{n}$ satisfies **i.i.d.**, its joint probability density function must satisfy:\n",
        "\n",
        "$$\n",
        "f(x_1, \\dots, x_n) = \\prod_{i=1}^n f(x_i)\n",
        "$$\n",
        "\n",
        "Now, returning to the OASIS dataset $\\mathcal{D}$, we need to examine whether the **observational units (Slices)** satisfy independence. Define $x_{i, s}$ and $x_{j, s}$ as two different MRI slice images from the same subject $s \\in \\mathcal{S}$, where $i \\neq j$. To test for independence, we need to utilize the properties of the covariance as follows:\n",
        "\n",
        "> **Theorem 4.5.5**\n",
        ">\n",
        "> If $X$ and $Y$ be independent random variables, then $\\mathrm{Cov}(X,Y) = 0$ and $\\rho_{XY} = 0$.\n",
        ">\n",
        "> $$\\mathrm{Cov}(X,Y) = E[XY] - E[X]E[Y] = E[X]E[Y] - E[X]E[Y] = 0$$\n",
        ">\n",
        "\n",
        "Stated differently, if $\\mathrm{Cov}(X, Y) \\neq 0$, then $X$ and $Y$ are **necessarily dependent**. The question now is:\n",
        "\n",
        "> *Can the observational unit $x \\in \\mathcal{X}$ be used as an **i.i.d.** sampling unit?*\n",
        ">\n",
        "\n",
        "Since the anatomical structure of the human brain exhibits a high degree of continuity and subject-specific characteristics, adjacent slices from the same subject will have significant **Intra-class Correlation** in the feature space, leading to $\\mathrm{Cov}(x_{i, s}, x_{j, s}) \\gg 0$. Mathematically, this means their joint expected value is not equal to the product of their marginal expected values, specifically:\n",
        "\n",
        "$$\n",
        "\\mathrm{Cov}(x_{i, s}, x_{j, s}) \\gg 0 \\implies E[x_{i, s} \\cdot x_{j, s}] \\neq E[x_{i, s}] \\cdot E[x_{j, s}]\n",
        "$$\n",
        "\n",
        "Therefore, if the **observational unit** $x \\in \\mathcal{X}$ (i.e., a single image slice) is directly treated as the sampling unit, the dataset $\\mathcal{D}$ **violates the independence condition of the i.i.d. assumption, rendering the WLLN inapplicable. The empirical risk** $\\hat{R}_n(f)$ **is no longer a consistent estimator, and all statistical guarantees of ERM are entirely void.**\n",
        "\n",
        "The statistical sampling methods can be categorized into:\n",
        "\n",
        "1. **Observational Unit** $x \\in \\mathcal{X}$: A single 2D MRI image, which is the minimal input unit for the model.\n",
        "2. **Experimental Unit** $s \\in \\mathcal{S}$: The patient (Subject)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
